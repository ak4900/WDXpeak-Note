网络爬虫，是一种按照一定的规则，自动的抓取万维网信息的程序或者脚本。网络爬虫是搜索引擎系统中十分重要的组成部分，它负责从互
联网中搜集网页，采集信息，这些网页信息用于建立索引从而为搜索 引擎提供支持，它决定着整个引擎系统的内容是否丰富，信息是否即
时，因此其性能的优劣直接影响着搜索引擎的效果。

  

网络爬虫程序的优劣，很大程度上反映了一个搜索引擎的好差。不信，你可以随便拿一个网站去查询一下各家搜索对它的网页收录情况，爬虫强大程度跟搜索引擎好坏基本成正比
。

  

**1.世界上最简单的爬虫——三行情诗**

  

我们先来看一个最简单的最简单的爬虫，用python写成，只需要三行。

>

>     import requests

>     url="http://www.cricode.com"

>     r=requests.get(url)

上面这三行爬虫程序，就如下面这三行情诗一般，很干脆利落。

  

是好男人，

就应该在和女友吵架时，

抱着必输的心态。

  

**2.一个正常的爬虫程序**

  

**上面那个最简单的爬虫，是一个不完整的残疾的爬虫。因为爬虫程序通常需要做的事情如下**：

  

1）给定的种子URLs，爬虫程序将所有种子URL页面爬取下来

2）爬虫程序解析爬取到的URL页面中的链接，将这些链接放入待爬取URL集合中

3）重复1、2步，直到达到指定条件才结束爬取

  

**因此，一个完整的爬虫大概是这样子的：**

>

>     import requests                       #用来爬取网页

>     from bs4 import BeautifulSoup         #用来解析网页

>     seds = ["http://www.hao123.com",      #我们的种子

>                   "http://www.csdn.net",

>                   "http://www.cricode.com"]

>     sum = 0                               #我们设定终止条件为：爬取到100000个页面时，就不玩了

>  
>     while sum < 10000 :

>         if sum < len(seds):

>              r = requests.get(seds[sum])

>              sum = sum + 1

>              do_save_action(r)

>              soup = BeautifulSoup(r.content)  
>              urls = soup.find_all("href",.....)                     //解析网页

>              for url in urls:

>                   seds.append(url)

>  
>         else:

>              break

**3.现在来找茬**

  

**上面那个完整的爬虫，不足20行代码，相信你能找出20个茬来。因为它的缺点实在是太多。下面一一列举它的N宗罪：**

  

1）我们的任务是爬取1万个网页，按上面这个程序，一个人在默默的爬取，假设爬起一个网页3秒钟，那么，爬一万个网页需要3万秒钟。MGD，我们应当考虑开启多个线程
(池)去一起爬取，或者用分布式架构去并发的爬取网页。

  

2）种子URL和后续解析到的URL都放在一个列表里，我们应该设计一个更合理的数据结构来存放这些待爬取的URL才是，比如队列或者优先队列。

  

3）对各个网站的url，我们一视同仁，事实上，我们应当区别对待。大站好站优先原则应当予以考虑。

  

4）每次发起请求，我们都是根据url发起请求，而这个过程中会牵涉到DNS解析，将url转换成ip地址。一个网站通常由成千上万的URL，因此，我们可以考虑将这
些网站域名的IP地址进行缓存，避免每次都发起DNS请求，费时费力。

  

5）解析到网页中的urls后，我们没有做任何去重处理，全部放入待爬取的列表中。事实上，可能有很多链接是重复的，我们做了很多重复劳动。

  

6）…..

  

**4.找了这么多茬后，很有成就感，真正的问题来了，学挖掘机到底哪家强？**

  

现在我们就来一一讨论上面找茬找出的若干问题的解决方案。

  

**1）并行爬起问题**

  

我们可以有多重方法去实现并行。

  

多线程或者线程池方式，一个爬虫程序内部开启多个线程。同一台机器开启多个爬虫程序，如此，我们就有N多爬取线程在同时工作。能大大减少时间。

  

此外，当我们要爬取的任务特别多时，一台机器、一个网点肯定是不够的，我们必须考虑分布式爬虫。常见的分布式架构有：主从（Master——Slave）架构、点对点
（Peer to Peer）架构，混合架构等。

  

说道分布式架构，那我们需要考虑的问题就有很多，我们需要分派任务，各个爬虫之间需要通信合作，共同完成任务，不要重复爬取相同的网页。分派任务我们要做到公平公正，
就需要考虑如何进行负载均衡。负载均衡，我们第一个想到的就是Hash，比如根据网站域名进行hash。

  

负载均衡分派完任务之后，千万不要以为万事大吉了，万一哪台机器挂了呢？原先指派给挂掉的哪台机器的任务指派给谁？又或者哪天要增加几台机器，任务有该如何进行重新分
配呢？

  

一个比较好的解决方案是用一致性Hash算法。

  

**2）待爬取网页队列**

  

如何对待待抓取队列，跟操作系统如何调度进程是类似的场景。

  

不同网站，重要程度不同，因此，可以设计一个优先级队列来存放待爬起的网页链接。如此一来，每次抓取时，我们都优先爬取重要的网页。

当然，你也可以效仿操作系统的进程调度策略之多级反馈队列调度算法。

  

**3）DNS缓存**

  

为了避免每次都发起DNS查询，我们可以将DNS进行缓存。DNS缓存当然是设计一个hash表来存储已有的域名及其IP。

  

**4）网页去重**

  

说到网页去重，第一个想到的是垃圾邮件过滤。垃圾邮件过滤一个经典的解决方案是Bloom Filter（布隆过滤器）。布隆过滤器原理简单来说就是：建立一个大的位
数组，然后用多个Hash函数对同一个url进行hash得到多个数字，然后将位数组中这些数字对应的位置为1。下次再来一个url时，同样是用多个Hash函数进行
hash，得到多个数字，我们只需要判断位数组中这些数字对应的为是全为1，如果全为1，那么说明这个url已经出现过。如此，便完成了url去重的问题。当然，这种
方法会有误差，只要误差在我们的容忍范围之类，比如1万个网页，我只爬取到了9999个，剩下那一个网页，who cares！

  

**5）数据存储的问题**

  

数据存储同样是个很有技术含量的问题。用关系数据库存取还是用NoSQL，抑或是自己设计特定的文件格式进行存储，都大有文章可做。

  

**6）进程间通信**

  

分布式爬虫，就必然离不开进程间的通信。我们可以以规定的数据格式进行数据交互，完成进程间通信。

  

**7）……**

  

废话说了那么多，真正的问题来了，问题不是学挖掘机到底哪家强？而是如何实现上面这些东西！：）

  

实现的过程中，你会发现，我们要考虑的问题远远不止上面这些。纸上得来终觉浅，觉知此事要躬行！

  

来自：快课网  
链接： ![](_resources/网页爬虫及其用到的算法和数据结构image0.)http://www.cricode.com/3622.html

  

—————————————————  

●本文编号465，以后想阅读这篇文章直接输入465即可。  

●本文分类“**搜索引擎技术**”，搜索分类名可以获得相关文章。

●输入m可以获取到全部文章目录

●输入r可以获取到热门文章推荐

●输入f可以获取到全部分类名称

—————————————————

**小猿个人微信：itcodemonkey 欢迎调戏**

**  
**

**推荐一个微信公众号：IT电商网，长按下面的微信号可以进行复制******

**itdianshang**

**  
**

**点击“阅读原文”可关注**

**  
**

![](_resources/网页爬虫及其用到的算法和数据结构image1.)

阅读原文

阅读

__ 举报

[阅读原文](http://mp.weixin.qq.com/s?__biz=MjM5NzA1MTcyMA==&mid=201800920&idx=3&sn
=a4526fae82380f4ae48a927fdae4409f&scene=0#rd)

