### 简介

过去几年，深度神经网络在模式识别中占绝对主流。它们在许多计算机视觉任务中完爆之前的顶尖算法。在语音识别上也有这个趋势了。

虽然结果好，我们也必须思考……它们为什么这么好使？

在这篇文章里，我综述一下在自然语言处理（NLP）上应用深度神经网络得到的一些效果极其显著的成果。我希望能提供一个能解释为何深度神经网络好用的理由。我认为这是
个非常简练而优美的视角。

### 单隐层神经网络

单隐层神经网络有一个普适性（universality）：给予足够的隐结点，它可以估算任何函数。这是一个经常被引用的理论，它被误解和应用的次数就更多了。

本质上这个理论是正确的，因为隐层可以用来做查询表。

简单点，我们来看一个感知器网络（perceptron network）。感知器
（perceptron）是非常简单的神经元，如果超过一个阈值它就会被启动，如果没超过改阈值它就没反应。感知器网络的输入和输出都是是二进制的（0和1）。

注意可能的输入个数是有限的。对每个可能的输入，我们可以在隐层里面构建一个只对这个输入有反应的神经元（见注解1）。然后我们可以利用这个神经元和输出神经元之间的
连接来控制这个输入下得到的结果（见注解2）。

![](_resources/深度学习、自然语言处理和表征方法image0.jpg)

这样可以说明单隐层神经网络的确是有普适性的。但是这也没啥了不起的呀。你的模型能干和查询表一样的事并不能说明你的模型有任何优点。这只能说明用你的模型来完成任务
并不是不可能的罢了。

普适性的真正意义是：一个网络能适应任何你给它的训练数据。这并不代表插入新的数据点的时候它能表现地很理想。

所以普适性并不能解释为什么神经网络如此好用。真正的原因比这微妙得多… 为了理解它，我们需要先理解一些具体的成果。

### 单词嵌入（Word Embeddings）

我想从深度学习研究的一个非常有意思的部分讲起，它就是：单词嵌入（word embeddings）。在我看来，单词嵌入是目前深度学习最让人兴奋的领域之一，尽管
它最早是由Bengio等人在十多年前提出的（见注解3）。除此之外，我认为它们能帮助你通过直觉来了解为什么深度学习如此有效。

单词嵌入W:words→Rn是一个参数化函数，它把某个语言里的单词映射成高维向量（大概200到500维）。例如这样：

>   * W(‘‘cat”)=(0.2, -0.4, 0.7, …)

>

>   * W(‘‘mat”)=(0.0, 0.6, -0.1, …)

（一般这个函数就是一个查询表，用一个矩阵θ来参数化，每行是一个单词：Wθ(wn)=θn.）  
初始化时，W中每个词对应一个随机的向量。它会学习出有意义的向量以便执行任务。  
举个一个可能的任务的例子：训练一个网络让其预测一个5元组（5-gram）（连续的5个词）是否‘成立’。我们可以随便从维基百科上选一堆5元组（比如cat
sat on the mat）然后把其中一个词随便换成另外一个词（比如cat sat song the
mat），那么一半的5元组估计都会变得荒谬且没意义了。

![](_resources/深度学习、自然语言处理和表征方法image12.jpg)判断5元组是否成立的模块网络

我们训练的模型会通过W把5元组中每个词的表征向量取出来，输入给另外一个叫R的模块，模块R会试图预测这个5元组是‘成立的’或者是‘破碎的’。然后我们希望看见:

>   * R(W(‘‘cat”), W(‘‘sat”), W(‘‘on”), W(‘‘the”), W(‘‘mat”))=1

>

>   * R(W(‘‘cat”), W(‘‘sat”), W(‘‘song”), W(‘‘the”), W(‘‘mat”))=0

为了准确地预测这些值，这个网络需要从W以及R中学习到好的参数。

现在看来这个任务并没什么意思。也许它能用来检测语法错误什么的，没什么大不了。但是极其有趣的部分是这个W。

（事实上，对我们来说，这个任务的意义就是学习W。我们当然也可以做一些其他的任务 –
一个很常见的任务是预测句子中下一个单词。但我们实际上并不在乎任务是什么。这节后面我们会谈到许多单词嵌入成果，但并不会区分得到这些成果的方法的不同。）

想直观感受一下单词嵌入空间的话，我们可以用t-SNE来对它进行可视化。t-SNE是一个复杂的高维数据可视化技术。

![](_resources/深度学习、自然语言处理和表征方法image2.jpg)t-SNE对单词嵌入的可视化结果。左图：数字区间。右图：工作岗位区间。

这种单词构成的“地图”对我们来说更直观。相似的词离得近。另一种方法是看对一个给定单词来说，哪些其他的单词离它最近。我们可以再一次看到，这些词都很相似。

![](_resources/深度学习、自然语言处理和表征方法image3.jpg)哪些词的嵌入离一个给定词最近？

网络能让意义相似的词拥有相似的向量，这看起来是很自然的事。如果你把一个词换成它的同义词(例如 “a few people sing well” → “a
couple people sing well”)，句子的成立性并没有变化。虽然从字面上看，句子变化很大，但如果W把同义词（像“few”和”couple”这
种）映射到相近的空间，从R的角度来看句子的变化很小。

这就牛了。可能的5元组的数目是巨大的，相比之下我们的训练数据量很小。相似的单词距离近能让我们从一个句子演变出一类相似的句子。这不仅指把一个词替换成一个它的同
义词，而且指把一个词换成一个相似类别里面的词（如“the wall is blue” → “the wall is red”
）。进一步地，我们可以替换多个单词（例如“the wall is blue” → “the ceiling is
red”）。它的影响对单词数目来说是指数级的 (参见注解4)。

很明显，这是W的一个用武之地。但它是如何学会做这个的呢？看起来很可能很多情况下它是先知道“the wall is
blue”这样的句子是成立的，然后才见到“the wall is red”这样的句子。这样的话，把“red”往”blue”那边挪近一点，网络的效果就更好。

我们并没见过每个单词使用的例子，但是类比能让我们泛化衍生出新的单词组合。你懂的单词你都见过，但是你能懂的句子你并没有都见过。神经网络也是如此。

![](_resources/深度学习、自然语言处理和表征方法image4.jpg)

单词嵌入展示了一个更引人注目的属性：单词间的类比仿佛是被编码在了单词向量的区别中。比如，这个看来是个男-女区别向量：

>   * W(‘‘woman”)−W(‘‘man”) ≃ W(‘‘aunt”)−W(‘‘uncle”)

>

>   * W(‘‘woman”)−W(‘‘man”) ≃ W(‘‘queen”)−W(‘‘king”)

也许这看起来并不奇怪。毕竟表性别的代词意味着换一个词整个句子的语法就错了。正常话是这么说的 “she is the aunt” ，“he is the
uncle.”。同样的，“he is the King”, “she is the Queen.”。如果你看见“she is the uncle,”
最可能的解释就是这句话有语法错误。这个情况看起来很可能是：一半的时候单词都被随机地替换了。

也许我们会放马后炮：“当然是这样啦！单词嵌入会学着把性别按照一致的方式来编码。事实上也许就存在一个性别的维度。对单复数来说也是一样。找出这些明显的关系太简单
了！”

然而，更复杂的关系也是这样被编码的。这看起来几乎像奇迹一样！

![](_resources/深度学习、自然语言处理和表征方法image5.jpg)单词嵌入中的关系对。

能够充分意识到W的这些属性不过是副产品而已是很重要的。我们没有尝试着让相似的词离得近。我们没想把类比编码进不同的向量里。我们想做的不过是一个简单的任务，比如
预测一个句子是不是成立的。这些属性大概也就是在优化过程中自动蹦出来的。

这看来是神经网络的一个非常强大的优点：它们能自动学习更好的数据表征的方法。反过来讲，能有效地表示数据对许多机器学习问题的成功都是必不可少的。单词嵌入仅仅是学
习数据表示中一个引人注目的例子而已。

### 共同表征

单词嵌入的这些属性当然非常有意思，但是除了判断5元组是不是成立这种傻问题还能干点啥有用的么？

![](_resources/深度学习、自然语言处理和表征方法image6.jpg)W和F学习完成任务A， G可以根据W来学习完成任务B

之前我们学习单词嵌入是为了在简单任务上有出色的表现，但基于我们从单词嵌入中发现的好属性，你也许会猜想它们对自然语言处理任务整体都适用。实际上，这样的单词特征
表示（word representations）是极其有用的：

> > “利用单词特征表示…已经成为近年来许多NLP系统成功的秘密武器，包括命名实体识别，词性标注，语法分析和语义角色标注。(Luong et al.
(2013) ”

在深度学习工具箱里，把从任务A中学到的好表征方法用在任务B上是一个很主要的技巧。根据细节不同，这个普遍的技巧的名称也不同，如：预训练（pretraining
），迁移学习(transfer learning)，多任务学习(multi-task
learning)等。这种方法的好处之一是可以从多种不同数据中学习特征表示。

这个技巧有个对应面。除了在一种数据上学习表征然后应用在不同任务上，我们还可以从多种数据中学习出一种单个的表征！

一个很好的例子就是Socher et al. (2013a)
提出的双语单词嵌入。我们可以从两种不同语言中把单词嵌入到一个共享的空间去。在这个例子里，我们学习把汉语和英语嵌入到同一个空间去。)

![](_resources/深度学习、自然语言处理和表征方法image7.jpg)

我们用和上面差不多的方法来训练Wen和Wzh两种嵌入。但是，我们已知某些中文和英文的词汇有相似的意思。所以，我们追加一个属性优化：我们已知的翻译过后意思相似
的词应该离得更近。

理所当然，我们会发现我们已知的有相似意思的词在最后结果中离得很近。我们本来就是针对这个做的优化，这个结果没什么让人惊讶的。但更有意思的是我们未知的翻译后意思
相似的词结果距离也很近。

鉴于我们前面有关单词嵌入的经验，这个也许并不太让你感到惊奇。单词嵌入就是会把相似的词聚到一起，所以如果我们已知的中英词汇离得近，它们的同义词自然离得近。我们
还知道类似性别差异趋向于可以用一个常数的差异向量表示。看起来，对齐足够多的点会让这些差异向量在中文和英文的嵌入中保持一致。这样会导致如果我们已知两个男性词互
为翻译，最后我们也会得到一对互为翻译的女性词。

直观来讲，仿佛就是两种语言有着相似的“形状”，通过对齐不同的点，两种语言就能够重叠，其他的点就自然能被放在正确的位置上。

![](_resources/深度学习、自然语言处理和表征方法image8.jpg)双语单词嵌入的t-SNE可视化图。绿色是中文，黄色是英文。

在双语单词嵌入中，我们对两种很相似的数据学习了一个共享表征。我们也可以学习把非常不同的几种数据嵌入到同一个空间去。

![](_resources/深度学习、自然语言处理和表征方法image9.jpg)

近期，深度学习已经开始探索能够把单词和图像嵌入到同一个表征下的模型（参见注解5）。

基本思路就是你可以通过单词嵌入输出的向量来对图像进行分类。狗的图像会被映射到“狗”的单词向量附近。马的图像会被映射到“马”的单词向量附近。汽车的图像会被映射
到“汽车”的单词向量附近。以此类推。

有趣的是如果你用新类别的图像来测试这个模型会发生什么呢？比如，如果这个模型没训练过如何分类“猫”，也就是把猫的图像映射到“猫”向量附近，那当我们试图对猫的图
像进行分类的时候会发生什么呢？

![](_resources/深度学习、自然语言处理和表征方法image10.jpg)

结果表明，这个网络是可以很合理地处理新类别的图像的。猫的图片并没有被映射到单词嵌入空间的随机的点中。相反的，他们更倾向于被映射到整体上相近的“狗”的向量中去
，并且事实上更接近于“猫”的向量。相似的，卡车的图片最后离“卡车”向量相对也比较近，“卡车”向量和与它相关的“汽车”向量很近。

![](_resources/深度学习、自然语言处理和表征方法image11.jpg)

这个图是斯坦福一个小组用8个已知类（和2个未知类别）做的图。结果已经很可观了。但因为已知类数目小，能够用来插入图像和语义空间的关系的点就很少了。

差不多同时期，Google的小组做了一个大得多的版本，他们用了1000个类别而不是8个(Frome et al.
(2013))。之后他们又做了一个新的版本(Norouzi et al.(2014))。两者都基于非常有效的图像分类模型(来自 Krizehvsky et
al.(2012))，但它们使用了不同的方式把图像嵌入到单词嵌入空间去。

他们的成果是很赞的。虽然他们不能把未知类的图片准确放到代表这个类的向量上去，但是他们能够把它放到正确的区域。所以，如果你用它来对区别比较大的未知类的图片来分
类，它是能够区分类别的不同的。

即使我从来没见过艾斯库拉普蛇和穿山甲，如果你给我看这两样东西的照片，我能告诉你哪个是哪个因为我大致知道这两个词和什么样的动物有关。这些网络可以做到同样的事情
。

（这些结果都利用到一种“这些词是相似的”的推断。但是看起来根据词之前的关系应该有更有力的结果。在我们的单词嵌入空间里，在男性和女性词上有一个一致的差异向量。
相似的，在图像空间中，也有一致的可以区分男性和女性的特征。胡子，八字胡，秃顶都是强烈的，可见的男性特征。胸部，及没那么可靠的如长发，化妆品及珠宝这些是明显的
女性特征（参见注解6）。即使你从来没见过一个国王，如果一个带着王冠的王后突然有了胡子，那把她变成男人也是很合理的。）

共享嵌入是一个非常让人兴奋的研究领域，它暗示着为何深度学习中这个注重表征方法的角度是如此的引人入胜。

### 递归神经网络

我们之前是用下面这个网络开始谈单词嵌入的：

![](_resources/深度学习、自然语言处理和表征方法image12.jpg)学习单词嵌入的模块化网络

上面的图描绘了一个模块化网络，R(W(w1), W(w2), W(w3), W(w4),
W(w5))。它是由两个模块构建的，W和R。这个用能拼在一起的小一些的神经网络模块来构建神经网络的方法传播并不是十分广泛。然而，在NLP中它很有效。

像上面那样的模型很有效，但很不幸它们有个局限：输入参数的个数必须是固定的。

![](_resources/深度学习、自然语言处理和表征方法image13.jpg)

我们可以通过加入一个关联模块A来解决这个问题。这个关联模块可以将两个单词或词组的表征合并起来。  
通过合并一系列的单词，A让我们不仅能够表示单词，而且能够表示词组甚至整个句子！另外因为我们可以合并不同数量的单词，我们就可以不固定死输入的个数了。

把句子中的单词线性地合并在一起的做法并不是在所有情况下都讲得通。考虑下面这个句子“the cat sat on the
mat”，很自然地它可以被分成下面这样用括号分开的不同的段:“((the cat) (sat (on (the mat))”.
我们可以把A应用在这个分段上：))

![](_resources/深度学习、自然语言处理和表征方法image14.jpg)

这样的模型通常被称作“递归神经网络”因为一个模块经常会使用另外一个同类型模块的输出。有时候它们也被称作“树形神经网络tree-structured
neural networks”。  
递归神经网络在一系列NLP任务中都有很重大的成功。比如Socher et al. (2013c) 就利用了一个递归神经网络来预测句子的情感：

![](_resources/深度学习、自然语言处理和表征方法image15.jpg)

一直以来，一个很主要的目标是如何创建一个可逆的句子表征（sentence
representation），也就是能够通过这个表征来构建一个真正的有着相似意思的句子。例如，我们可以尝试引入一个分解模块（disassociation
module）D来试着把A分解了：

![](_resources/深度学习、自然语言处理和表征方法image16.jpg)

如果这个能成功，将会是一个极其强大的工具。举个例子，我们可以尝试做一个双语句子表征然后把它用在翻译任务上。

不幸的是，这个实际上是很难实现的。非常，非常难。同时因为它一旦成功有巨大的前途，有很多人在为研究它而努力。

最近，Cho et al. (2014)在词组表征上有了一些进展，他们做了一个能把英语词组编码，解码成法语的模型。来看看它学习出来的词组表征吧！

![](_resources/深度学习、自然语言处理和表征方法image17.jpg)词组表征的t-SNE的一小部分

### 批判

有关上面我们综述的一些结果，我也听说有其他领域的研究人员，尤其是NLP和语言学的人，对他们进行了批判。他们的顾虑倒不是针对结果本身的，反而是从结果中得出的结
论以及他们和其他方法的区别。

我觉得自己的能力不足以清晰的表达这些顾虑。我鼓励有能力的人在（英文原文）评论里描述这些顾虑。

### 结论

深度学习中的表征视角是非常有力的，也似乎能够解答为何深度神经网络如此有效。在此之上，我认为它还有一个极美的地方：为何神经网络有效？因为在优化多层模型的过程中
，更好的来数据表征方法会自动浮现出来。

深度学习是个非常年轻的领域，理论根基还不强，观点也在快速地改变。我感觉神经网络中重视表征的这个方面目前是十分流行的。

在这篇文章里，我综述了一些我觉得十分让人兴奋的研究成果，但我写这篇文章的主要动力是为之后要写的一篇探索深度学习，类型论（type
theory）和功能性编程（functional
programming）之间关系的文章铺路。如果你感兴趣的话，可以订阅我的RSS（原文作者），这样文章发布时你就能看见了。

（我很乐意听听你们的想法和评论。如果针对英文原文你发现了错别字，技术错误，或者你认为需要添加的修正或者澄清，欢迎到github来pull。）

### 致谢

我很感激Eliana Lorch、Yoshua Bengio、Michael Nielsen、Laura Ball、Rob Gilson 及 Jacob
Steinhardt 的评论和支持。

### 注解：

  1. 当你有n个输入神经元时，构建所有可能的输入情况需要2^n个隐神经元。在实际操作中，通常不会这么严重。你可以采取能够包含不同输入的情况。你也可以采用重叠的情况，他们利用叠加的方式来在交集处获得正确的输入。

  2. 不仅是感知器网络才有普适性。多层感知器（sigmoid neurons）网络（及其他激发函数）也具有普适性：给予足够的隐节点，他们估算任何连续函数都可以得到不错的结果。因为你不能简单地孤立输入，所以想看明白这点是十分复杂的。

  3. 单词嵌入最初是由(Bengio et al, 2001; Bengio et al, 2003)开发的。那是2006年深度学习重构开始的前几年，那时神经网络被认为是过时的。而符号话的向量表示（distributed representations）的概念就更老了，比如(Hinton 1986)。

  4. 这篇开创性的文章：A Neural Probabilistic Language Model (Bengio, et al. 2003)里包含了很多单词嵌入为何有力的解释。

  5. 之前也有对图像和标签联合分布建模的工作，但他们的观点和我们要描述的截然不同。

  6. 我十分清楚利用性别的外表特征可能是十分误导人的。来暗示诸如每个秃头的人都是男性或者每个有胸部的人都是女性并不是我的本意。只是这些是通常的情况而已，而它们可以用来很大程度上的调节我们的先验知识。

  

阅读原文

阅读

举报

[阅读原文](http://mp.weixin.qq.com/s?__biz=MjM5ODIzNDQ3Mw==&mid=200670389&idx=1&sn
=5187404aab3962eb0880ee02d4daa059&scene=0#rd)

